{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IIsc_Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/swastikmhptr/swastikmhptr.github.io/blob/master/IIsc_Assignment.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "bGqxMABbSSyJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Fizz Buzz Logic ##\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ssYTWt5PR5_4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model To Classify a number according to Fizz-Buzz Logic##\n",
        "\n",
        "Here i have implemented the complete Neural netowrk from scratch with the parameters specified and while only importing Numpy for fast operation"
      ]
    },
    {
      "metadata": {
        "id": "YR_NAo_S4rmz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Libraries used:-**\n",
        "- numpy is the fundamental package for scientific computing with Python..\n",
        "- matplotlib is a famous library to plot graphs in Python.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "G6bVIiWIR5_7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m2y4XWMz5Llu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Creation Of The Fizz-Buzz Dataset**"
      ]
    },
    {
      "metadata": {
        "id": "SMWq-Jl0E0Wq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "1079e27d-360d-411b-9265-3719f9a0f08a"
      },
      "cell_type": "code",
      "source": [
        "arr = np.zeros((1000,13))\n",
        "p = 1;\n",
        "for i in range(1000):\n",
        "  arr[i][0]=p\n",
        "  if (p%3==0 and p%5==0):\n",
        "    arr[i][1]=3\n",
        "  elif p%3==0:\n",
        "    arr[i][1]=1\n",
        "  elif p%5==0:\n",
        "    arr[i][1]=2\n",
        "  else:\n",
        "    arr[i][1]=0\n",
        "  \n",
        "  arr[i][2]=np.base_repr(p)\n",
        "  q=arr[i][2]\n",
        "  for j in range(12,2,-1):\n",
        "    arr[i][j]=np.around(q%10)\n",
        "    q=q/10\n",
        "  p=p+1\n",
        "print(arr)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.00000000e+00 0.00000000e+00 1.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00]\n",
            " [2.00000000e+00 0.00000000e+00 1.00000000e+01 ... 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00]\n",
            " [3.00000000e+00 1.00000000e+00 1.10000000e+01 ... 0.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00]\n",
            " ...\n",
            " [9.98000000e+02 0.00000000e+00 1.11110011e+09 ... 1.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00]\n",
            " [9.99000000e+02 1.00000000e+00 1.11110011e+09 ... 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00]\n",
            " [1.00000000e+03 2.00000000e+00 1.11110100e+09 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J9quHAtn5V2Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now since the dataset is created, columns 1 and 3 are removed"
      ]
    },
    {
      "metadata": {
        "id": "piW5QbUME6Jv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d456adca-bb0a-46ef-ca6f-6612fad3a32e"
      },
      "cell_type": "code",
      "source": [
        "arr1=np.zeros((1000,11))\n",
        "arr1=np.delete(arr,[0,2], 1)\n",
        "\n",
        "print(\"final array \\n\")\n",
        "print(arr1)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final array \n",
            "\n",
            "[[0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [1. 0. 0. ... 0. 1. 1.]\n",
            " ...\n",
            " [0. 1. 1. ... 1. 1. 0.]\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " [2. 1. 1. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ElbVyGds5A34",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Splitting the data into required Train and Validation datasets**"
      ]
    },
    {
      "metadata": {
        "id": "E74kWO7KzCNm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "split = 100\n",
        "df = arr1[:split,:]\n",
        "dt = arr1[split:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g90M_w0C0FnE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "4b7d85cf-b3c6-4cae-eb6d-291d3379a697"
      },
      "cell_type": "code",
      "source": [
        "train = np.zeros((900,11))\n",
        "train[:] = dt[:]\n",
        "print(train)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 1. 0. 1.]\n",
            " [1. 0. 0. ... 1. 1. 0.]\n",
            " [0. 0. 0. ... 1. 1. 1.]\n",
            " ...\n",
            " [0. 1. 1. ... 1. 1. 0.]\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " [2. 1. 1. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sjGpJIeV0Fwy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "fdc46a1d-8e88-44fa-b542-2ce3b64eaa98"
      },
      "cell_type": "code",
      "source": [
        "test = np.zeros((100,11))\n",
        "test[:] = df[:]\n",
        "print(test)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [1. 0. 0. ... 0. 1. 1.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [1. 0. 0. ... 0. 1. 1.]\n",
            " [2. 0. 0. ... 1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z0s7nbiM03NP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3557302e-38cc-439a-c729-eb5cad53eee3"
      },
      "cell_type": "code",
      "source": [
        "print(\"test\"+ str(test.shape))\n",
        "print(\"train\"+ str(train.shape))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test(100, 11)\n",
            "train(900, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dmcdHaAUSSy1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**separating the prediction value y from train and test datasets**"
      ]
    },
    {
      "metadata": {
        "id": "8XRgafFG03Xw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1df234da-63c0-4d7b-e5b4-cc1b4d18fa3b"
      },
      "cell_type": "code",
      "source": [
        "Y_test1 = np.zeros((100,1))\n",
        "\n",
        "Y_test1[:] = test[:,[0]]\n",
        "Y_test2 = np.copy(Y_test1.T.astype(int))\n",
        "print(Y_test2)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1\n",
            "  0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1\n",
            "  0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KfLJ651xI9Zj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**One-Hot Encoding the Test Target feature**"
      ]
    },
    {
      "metadata": {
        "id": "l0ZZD62g5zl2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        },
        "outputId": "18494158-519c-4a8c-83d5-67c6a46ee727"
      },
      "cell_type": "code",
      "source": [
        "Y_test = np.zeros((100,4))\n",
        "Y_test[np.arange(100),Y_test2]=1\n",
        "print(Y_test)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JwbhfXOi03fk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "d9bade81-9ada-4bb7-d5f5-3fbb3f9df0d4"
      },
      "cell_type": "code",
      "source": [
        "Y_train1 = np.zeros((900,1))\n",
        "\n",
        "Y_train1[:] = train[:,[0]]\n",
        "Y_train2 = np.copy(Y_train1.T.astype(int))\n",
        "print(Y_train2)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0\n",
            "  0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0\n",
            "  0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0\n",
            "  0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0\n",
            "  2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2\n",
            "  0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0\n",
            "  0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0\n",
            "  0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0\n",
            "  0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0\n",
            "  2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2\n",
            "  0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0\n",
            "  0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0\n",
            "  0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0\n",
            "  0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0\n",
            "  2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2\n",
            "  0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0\n",
            "  0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0\n",
            "  0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0\n",
            "  0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0\n",
            "  2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2\n",
            "  0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0\n",
            "  0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0\n",
            "  0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0\n",
            "  0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0\n",
            "  2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8ahmv2dyJYRc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**ONe Hot Encoding the Train Target Feature**"
      ]
    },
    {
      "metadata": {
        "id": "f-4Jx61s7AAl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "3a20a404-7da1-4877-f420-bfeb97aaea26"
      },
      "cell_type": "code",
      "source": [
        "Y_train = np.zeros((900,4))\n",
        "Y_train[np.arange(900),Y_train2]=1\n",
        "print(Y_train)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BO_9ApW5Jhl5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Deleting the Y values from the X-train and X-Test**"
      ]
    },
    {
      "metadata": {
        "id": "m7YE91ax03l0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test = np.zeros((100,10))\n",
        "X_train = np.zeros((900,10))\n",
        "\n",
        "X_test = np.delete(test,0,axis=1)\n",
        "X_train = np.delete(train,0,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D5lUD6aQ1c-Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1870
        },
        "outputId": "f8f53b7a-acf7-4c25-a565-be5e9867be11"
      },
      "cell_type": "code",
      "source": [
        "print(X_test)\n",
        "print(\"test:\"+ str(X_test.shape))\n",
        "print(X_train)\n",
        "print(\"Train:\"+ str(X_train.shape))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 1. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 1. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 1. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 1. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 1. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 1. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 1. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 1. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 1. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 1. 0. 1. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 1. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 1. 1. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 1. 1. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 1. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 1. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
            " [0. 0. 0. 0. 1. 1. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 1. 0. 1. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 1. 0. 1. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 1. 0. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 1. 1. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
            " [0. 0. 0. 0. 1. 1. 1. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 1. 1. 1. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 1. 1.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 1. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 1. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
            " [0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0. 1. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 1. 0. 1. 1.]\n",
            " [0. 0. 0. 1. 0. 0. 1. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 1. 1. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0. 1. 1. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 1. 1. 1. 1.]\n",
            " [0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 1. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 1. 0. 0. 1. 1.]\n",
            " [0. 0. 0. 1. 0. 1. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 1. 0. 1. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 1. 0. 1. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 1. 0. 1. 1. 1.]\n",
            " [0. 0. 0. 1. 0. 1. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 1. 1. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 1. 1. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 1. 1. 0. 1. 1.]\n",
            " [0. 0. 0. 1. 0. 1. 1. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 1. 1. 1. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 1. 1. 1. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 1. 1. 1. 1. 1.]\n",
            " [0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 1. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 1. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 1. 0. 0. 0. 1. 1.]\n",
            " [0. 0. 0. 1. 1. 0. 0. 1. 0. 0.]]\n",
            "test:(100, 10)\n",
            "[[0. 0. 0. ... 1. 0. 1.]\n",
            " [0. 0. 0. ... 1. 1. 0.]\n",
            " [0. 0. 0. ... 1. 1. 1.]\n",
            " ...\n",
            " [1. 1. 1. ... 1. 1. 0.]\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 0. 0. 0.]]\n",
            "Train:(900, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LyQPq5rzSSzK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Transposing the final datasets**"
      ]
    },
    {
      "metadata": {
        "id": "gEM6zq9t1dKc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "X_train_final = np.zeros((10,900))\n",
        "X_test_final = np.zeros((10,100))\n",
        "Y_train_final = np.zeros((4,900))\n",
        "Y_test_final = np.zeros((4,100))\n",
        "\n",
        "X_train_final = np.transpose(X_train)\n",
        "X_test_final = np.transpose(X_test)\n",
        "Y_train_final = np.transpose(Y_train)\n",
        "Y_test_final = np.transpose(Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L19Rh_wgO3a9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "7569ea4e-60a3-4473-a417-4b58e23bc858"
      },
      "cell_type": "code",
      "source": [
        "print(X_train_final[:][:])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 1. 1. 1.]\n",
            " [0. 0. 0. ... 1. 1. 1.]\n",
            " [0. 0. 0. ... 1. 1. 1.]\n",
            " ...\n",
            " [1. 1. 1. ... 1. 1. 0.]\n",
            " [0. 1. 1. ... 1. 1. 0.]\n",
            " [1. 0. 1. ... 0. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K-C_8CFkSSzR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**NORMALISING the training and Validation data**"
      ]
    },
    {
      "metadata": {
        "id": "Xhx4F4_AKoCz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here Normalization has been skipped because the complete dataset is made up of boolean values"
      ]
    },
    {
      "metadata": {
        "id": "Q8ZPyZew1dTU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#mtr = X_train_final.shape[1]\n",
        "#print(mtr)\n",
        "#utr = np.sum(X_train_final,axis=1,keepdims=True)/mtr\n",
        "#print(utr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yqDS4cDp1diK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#train_set_x = X_train_final - utr\n",
        "#test_set_x = X_test_final - utr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wVlbc9X_6Vh-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#norm = (np.sum(np.square(train_set_x),axis=1,keepdims=True))/mtr\n",
        "\n",
        "#train_set_x = train_set_x / norm\n",
        "#test_set_x = test_set_x / norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p18BMJqJ6Vwd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print(train_set_x)\n",
        "#print(\"train:\"+ str(train_set_x.shape))\n",
        "#print(test_set_x)\n",
        "#print(\"Test:\"+ str(test_set_x.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "97gGiemK88ke",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Activation Functions"
      ]
    },
    {
      "metadata": {
        "id": "VsFd3Ql28K1T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Defininhg Sigmoid activation function**"
      ]
    },
    {
      "metadata": {
        "id": "s4hrQCzwR6Af",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "   \n",
        "    s = 1 / (1 + np.exp(-z))\n",
        "    \n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kvmKLSySePc4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoiderivative(Z):\n",
        "  \n",
        "  f = Z * (1-Z)\n",
        "  \n",
        "  return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pxopqj7P9Jk_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**function for tanh activation**"
      ]
    },
    {
      "metadata": {
        "id": "rFopXuTKR6Aj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tanh(z):\n",
        "    \n",
        "    s = (np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z))\n",
        "    \n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sODSXD3t9O8V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**derivative of tanh activation**"
      ]
    },
    {
      "metadata": {
        "id": "avRK7nqTR6Al",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tanhderivative(x):\n",
        "    \n",
        "    p = 1 - np.square(x)\n",
        "    \n",
        "    return p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SP9p2N4T9T45",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**function for RELU activation**"
      ]
    },
    {
      "metadata": {
        "id": "iA51_1fcR6Ap",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def relu(z):\n",
        "    \n",
        "    s = np.maximum(0,z)\n",
        "    \n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "98b_bWNt9ZXu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**derivative of RELU activation**"
      ]
    },
    {
      "metadata": {
        "id": "tlAHT-xmR6Aq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def reluderivative(x):\n",
        "    x[x<=0] = 0\n",
        "    x[x>0] = 1\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9PeAKKK05jZI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next the softmax function is defined which helps in the multi-class classification as we have for this problem"
      ]
    },
    {
      "metadata": {
        "id": "Ba9vjtnC8kvK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x)\n",
        "    return np.exp(x) / np.sum(e_x,axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uGxL018z9obo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Core functions"
      ]
    },
    {
      "metadata": {
        "id": "JzDll9W89su-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Initialising the Weights and bais with very small values **"
      ]
    },
    {
      "metadata": {
        "id": "yPiutUqpR6Au",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initialize(layer):\n",
        "    \n",
        "    l = len(layer)\n",
        "    parameters = {}\n",
        "    np.random.seed(0)\n",
        "    for i in range(1,l-1):\n",
        "      parameters[\"W\"+str(i)]=np.random.randn(layer[i],layer[i-1]) * np.sqrt(2/layer[i-1]) \n",
        "      parameters[\"b\"+str(i)]=np.zeros((layer[i],1))\n",
        "    \n",
        "    parameters[\"W\"+str(l-1)]=np.random.randn(layer[l-1],layer[l-2]) * 0.001\n",
        "    parameters[\"b\"+str(l-1)]=np.zeros((layer[l-1],1)) \n",
        "   \n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OOPLfsvVSS0U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Initialising parameters for ADAM Optimiser**"
      ]
    },
    {
      "metadata": {
        "id": "e4cXpCkfLXTE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Just for the cases if application of Mini Batch Gradient Descent is applied"
      ]
    },
    {
      "metadata": {
        "id": "tJcmhrTWwNU8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initialize_adam(parameters,layer) :\n",
        "    \n",
        "    L = len(parameters) // 2\n",
        "    v = {}\n",
        "    s = {}\n",
        "    \n",
        "   \n",
        "    for l in range(L-1):\n",
        "   \n",
        "        v[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
        "        v[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
        "        s[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
        "        s[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
        "    \n",
        "    \n",
        "    return v, s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4IjvbSL6SS0W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Breaking the training dataset with random minibatches**"
      ]
    },
    {
      "metadata": {
        "id": "9DpBeA5jM8ff",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This function has been used to split our given dataset of 1000 entries into smaller batches for min-Batch Gradient Descent if needed"
      ]
    },
    {
      "metadata": {
        "id": "lAk0CFNTwNiz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def random_mini_batches(X, Y, mini_batch_size = 512):\n",
        "    \n",
        "               \n",
        "    m = X.shape[1]                  \n",
        "    mini_batches = []\n",
        "        \n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[:, permutation]\n",
        "    shuffled_Y = Y[:, permutation].reshape((4,m))\n",
        "\n",
        "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
        "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        mini_batch_X = shuffled_X[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
        "        mini_batch_Y = shuffled_Y[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
        "      \n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    # Handling the end case (last mini-batch < mini_batch_size)\n",
        "    if m % mini_batch_size != 0:\n",
        "        \n",
        "        mini_batch_X = shuffled_X[:, num_complete_minibatches*mini_batch_size:]\n",
        "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches*mini_batch_size:]\n",
        "        \n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    return mini_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rlA0IH7-936C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**FORWARD PROPAGATION**"
      ]
    },
    {
      "metadata": {
        "id": "FnEOagzmNRZk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function for general Forward propagation without application of dropouts"
      ]
    },
    {
      "metadata": {
        "id": "rbWSNzajkZ9w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fwd_propagation(X,parameters,layer):\n",
        "  \n",
        "  l = len(layer)\n",
        "  forward = {}\n",
        "  \n",
        "  forward[\"Z\"+str(1)]=np.dot(parameters[\"W\"+str(1)],X)+parameters[\"b\"+str(1)]\n",
        "    \n",
        "  forward[\"A\"+str(1)]=relu(forward[\"Z\"+str(1)])\n",
        "  \n",
        "  for i in range(2,l-1):\n",
        "    forward[\"Z\"+str(i)]=np.dot(parameters[\"W\"+str(i)],forward[\"A\"+str(i-1)])+parameters[\"b\"+str(i)]\n",
        "    forward[\"A\"+str(i)]=relu(forward[\"Z\"+str(i)])\n",
        "   \n",
        "  forward[\"Z\"+str(l-1)]=np.dot(parameters[\"W\"+str(l-1)],forward[\"A\"+str(l-2)])+parameters[\"b\"+str(l-1)]\n",
        "  forward[\"A\"+str(l-1)]=sigmoid(forward[\"Z\"+str(l-1)])\n",
        "  \n",
        "  return forward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vu6enn7FSS0c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**FORWARD-PROPAGATION with DROPOUTS**"
      ]
    },
    {
      "metadata": {
        "id": "9zw8AGQjNZKj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function to implement forward propagation with the application of dropouts"
      ]
    },
    {
      "metadata": {
        "id": "NbcDkA9VYp3v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fwd_propagation_drop(X,parameters,layer,keep_prob):\n",
        "  drop = {}\n",
        "  l = len(layer)\n",
        "  forward = {}\n",
        "  \n",
        "  forward[\"Z\"+str(1)]=np.dot(parameters[\"W\"+str(1)],X)+parameters[\"b\"+str(1)]\n",
        "    \n",
        "  forward[\"A\"+str(1)]=relu(forward[\"Z\"+str(1)])\n",
        "  drop[\"d\"+str(1)]=(np.random.rand(forward[\"A\"+str(1)].shape[0],forward[\"A\"+str(1)].shape[1])) < keep_prob[1]\n",
        "    \n",
        "  forward[\"A\"+str(1)] = np.multiply(forward[\"A\"+str(1)],drop[\"d\"+str(1)])\n",
        "  forward[\"A\"+str(1)] = forward[\"A\"+str(1)] / keep_prob[1]\n",
        "  \n",
        "  for i in range(2,l-1):\n",
        "    forward[\"Z\"+str(i)]=np.dot(parameters[\"W\"+str(i)],forward[\"A\"+str(i-1)])+parameters[\"b\"+str(i)]\n",
        "    \n",
        "    forward[\"A\"+str(i)]=relu(forward[\"Z\"+str(i)])\n",
        "    drop[\"d\"+str(i)]=(np.random.rand(forward[\"A\"+str(i)].shape[0],forward[\"A\"+str(i)].shape[1])) < keep_prob[i]\n",
        "    \n",
        "    forward[\"A\"+str(i)] = np.multiply(forward[\"A\"+str(i)],drop[\"d\"+str(i)])\n",
        "    forward[\"A\"+str(i)] = forward[\"A\"+str(i)] / keep_prob[i]\n",
        "    \n",
        "  forward[\"Z\"+str(l-1)]=np.dot(parameters[\"W\"+str(l-1)],forward[\"A\"+str(l-2)])+parameters[\"b\"+str(l-1)]\n",
        "  forward[\"A\"+str(l-1)]=softmax(forward[\"Z\"+str(l-1)])\n",
        "  \n",
        "  return forward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_epDQmAQSS0f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**BACK_PROPATION WITH L2 Regularization**"
      ]
    },
    {
      "metadata": {
        "id": "ohLBZs_INhgq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This function is tackle the case when there is less validation accuracy"
      ]
    },
    {
      "metadata": {
        "id": "W2fH8ImOb-cj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def back_prop(X,Y,forward,layer,parameters,lambd=0.6):\n",
        "  m = X.shape[1]\n",
        "  l = len(layer)\n",
        "  grads = {}\n",
        "  \n",
        "  grads[\"dz\"+str(l-1)]= forward[\"A\"+str(l-1)] - Y\n",
        "  grads[\"dw\"+str(l-1)]=(np.dot(grads[\"dz\"+str(l-1)],forward[\"A\"+str(l-2)].T) / m) + ((lambd/m)*parameters[\"W\"+str(l-1)])\n",
        "  grads[\"db\"+str(l-1)]=np.sum(grads[\"dz\"+str(l-1)],axis=1,keepdims=True) / m \n",
        "  grads[\"da\"+str(l-2)]=np.dot(parameters[\"W\"+str(l-1)].T,grads[\"dz\"+str(l-1)])\n",
        "  \n",
        "  for i in range(l-2,1,-1):\n",
        "    grads[\"dz\"+str(i)]=grads[\"da\"+str(i)]*reluderivative(forward[\"A\"+str(i)])\n",
        "    grads[\"dw\"+str(i)]=(np.dot(grads[\"dz\"+str(i)],forward[\"A\"+str(i-1)].T) / m) + ((lambd/m)*parameters[\"W\"+str(i)])\n",
        "    grads[\"db\"+str(i)]=np.sum(grads[\"dz\"+str(i)],axis=1,keepdims=True)/m\n",
        "    grads[\"da\"+str(i-1)]=np.dot(parameters[\"W\"+str(i)].T,grads[\"dz\"+str(i)])\n",
        "  \n",
        "  grads[\"dz\"+str(1)]=grads[\"da\"+str(1)]*reluderivative(forward[\"A\"+str(1)])\n",
        "  grads[\"dw\"+str(1)]=np.dot(grads[\"dz\"+str(1)],X.T)/m + ((lambd/m)*parameters[\"W\"+str(1)])\n",
        "\n",
        "  grads[\"db\"+str(1)]=np.sum(grads[\"dz\"+str(1)],axis=1,keepdims=True)/m\n",
        "    \n",
        "  return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lVuaLATN9_-W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**function to update the weights and Bais**"
      ]
    },
    {
      "metadata": {
        "id": "uFTDqzihNzJA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Gradient Descent Updation of Weights and Bias"
      ]
    },
    {
      "metadata": {
        "id": "Yim9YcQYhsXp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def optimise(parameters,grads,layer,learning_rate=0.01):\n",
        "  l= len(layer)\n",
        "  \n",
        "  for i in range(1,l):\n",
        "    parameters[\"W\"+str(i)]=parameters[\"W\"+str(i)] - (learning_rate * grads[\"dw\"+str(i)])\n",
        "    parameters[\"b\"+str(i)]=parameters[\"b\"+str(i)] - (learning_rate * grads[\"db\"+str(i)])\n",
        "  \n",
        "  return parameters\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZQQTtWF4SS0m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Updating parameters with ADAM optimizer**"
      ]
    },
    {
      "metadata": {
        "id": "G1UAB5qzN_3y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function to optimise parameters if adams Optimizer is choosen to be used"
      ]
    },
    {
      "metadata": {
        "id": "FKKmydKsxC0y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def update_parameters_with_adam(parameters, grads, layer, v, s, t, learning_rate = 0.01, beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
        "    \n",
        "    \n",
        "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
        "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
        "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
        "    \n",
        "    \n",
        "    for l in range(L-1):\n",
        "       \n",
        "        v[\"dW\" + str(l+1)] = beta1 * v['dW' + str(l+1)] + (1-beta1) * grads['dw' + str(l+1)]\n",
        "        v[\"db\" + str(l+1)] = beta1 * v['db' + str(l+1)] + (1-beta1) * grads['db' + str(l+1)]\n",
        "       \n",
        "        v_corrected[\"dw\" + str(l+1)] = v['dW' + str(l+1)] / (1 - np.power(beta1, t))\n",
        "        v_corrected[\"db\" + str(l+1)] = v['db' + str(l+1)] / (1 - np.power(beta1, t))\n",
        "       \n",
        "\n",
        "        s[\"dW\" + str(l+1)] = beta2 * s['dW' + str(l+1)] + (1-beta2) * np.power(grads['dw' + str(l+1)], 2)\n",
        "        s[\"db\" + str(l+1)] = beta2 * s['db' + str(l+1)] + (1-beta2) * np.power(grads['db' + str(l+1)], 2)\n",
        "        \n",
        "        s_corrected[\"dw\" + str(l+1)] = s['dW' + str(l+1)] / (1 - np.power(beta2, t))\n",
        "        s_corrected[\"db\" + str(l+1)] = s['db' + str(l+1)] / (1 - np.power(beta2, t))\n",
        "       \n",
        "        parameters[\"W\" + str(l+1)] = parameters['W' + str(l+1)] - learning_rate * v_corrected['dw' + str(l+1)] / np.sqrt(s_corrected['dw' + str(l+1)] + epsilon)\n",
        "        parameters[\"b\" + str(l+1)] = parameters['b' + str(l+1)] - learning_rate * v_corrected['db' + str(l+1)] / np.sqrt(s_corrected['db' + str(l+1)] + epsilon)\n",
        "       \n",
        "    return parameters, v, s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ERcdNlRc-K7b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**function to predict result**"
      ]
    },
    {
      "metadata": {
        "id": "__pDc33RjqMd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(X,parameters,layers):\n",
        "  l= len(layer)\n",
        "  fwd = fwd_propagation(X,parameters,layers)\n",
        "  \n",
        "  Y1 = fwd[\"A\"+str(l-1)]\n",
        "  Y2=np.argmax(Y1,axis=0)\n",
        "  \n",
        "  return Y2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ujf3LJhHSS0u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Function to retieve total cost for an epoch using cross-entropy loss function**"
      ]
    },
    {
      "metadata": {
        "id": "oeyoj2FMk-ss",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calculate_cost(Y,forward,parameters,layer,lambd=0.6):\n",
        "  l= len(layer)\n",
        "  m=Y.shape[1]\n",
        "  \n",
        "  cost = (-1 * np.sum((Y * np.log(forward[\"A\"+str(l-1)])) + (1 - Y) * np.log(1 - forward[\"A\"+str(l-1)])) / m) + ((lambd/(2*m))*np.sum(np.square(parameters[\"W\"+str(l-1)])))\n",
        "  \n",
        "  return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J6UPvx3f-WfN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Final model with all functions merged**"
      ]
    },
    {
      "metadata": {
        "id": "002kCX9NnC2A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model(X_train,Y_train, X_test, Y_test, layer, keep_prob,mini_batch_size=512, beta1=0.0, beta2=0.0, learning_rate=0.001, iterations = 1500, lambd=0.6):\n",
        "  costs = []\n",
        "  \n",
        "  parameters = initialize(layer)\n",
        "  v, s = initialize_adam(parameters,layer)\n",
        "  t = 0\n",
        "  \n",
        "  for j in range(iterations):\n",
        "  \n",
        "    minibatches = random_mini_batches(X_train,Y_train,mini_batch_size)\n",
        "    \n",
        "    for minibatch in minibatches:\n",
        "      (minibatch_x,minibatch_y) = minibatch\n",
        "      \n",
        "      forward = fwd_propagation_drop(minibatch_x,parameters,layer,keep_prob)\n",
        "      \n",
        "      cost = calculate_cost(minibatch_y,forward,parameters,layer,lambd)\n",
        "      grads = back_prop(minibatch_x,minibatch_y,forward,layer,parameters,lambd)\n",
        "      t=t+1\n",
        "      parameters,v,s = update_parameters_with_adam(parameters, grads, layer, v, s, t, learning_rate,beta1, beta2)\n",
        "    \n",
        "    \n",
        "    if j % 10 == 0:\n",
        "      costs.append(cost)\n",
        "    \n",
        "    if j%100 == 0:\n",
        "      print (\"Cost after iteration %i: %f\" %(j , cost))\n",
        "    #if j == 100:\n",
        "      #par = parameters[:]\n",
        "      \n",
        "  Y_prediction_train = predict(X_train,parameters,layer)\n",
        "  Y_prediction_test = predict(X_test,parameters,layer)\n",
        "\n",
        "  #abb = np.mean(np.abs(Y_prediction_train - Y_train2)) * 100\n",
        "  #abc = np.mean(np.abs(Y_prediction_test - Y_test2)) * 100\n",
        " # print(\"train accuracy: {} %\".format(100 - abb))\n",
        " # print(\"test accuracy: {} %\".format(100 - abc))\n",
        "  \n",
        "  return parameters, grads, forward, costs, Y_prediction_test, Y_prediction_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a0_hVeqpR6A_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training of model"
      ]
    },
    {
      "metadata": {
        "id": "hRNgLaTqqGrW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The Model is trained with two scenarios:-**\n",
        "\n",
        "**1) First one is as per the parameter values provided in the assignment**\n",
        "\n",
        "**2) Second is the most optimal found results**"
      ]
    },
    {
      "metadata": {
        "id": "IWyEiRQoqGmQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Scenario1\n",
        "\n",
        "**Here as per the parameter values given we have taken only one hidden layer with 100 neurons and no dropouts or Regularization is done, it also follows the whole batck gradient Descent Approach**"
      ]
    },
    {
      "metadata": {
        "id": "zuovrTcKqYD9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3757
        },
        "outputId": "df87b262-c30f-47ae-9337-060ed70cfc43"
      },
      "cell_type": "code",
      "source": [
        "layer = [10,100,4] #to set number of layers and the number of nodes in hidden layers (here there is only one hidden layer with 100 neurons)\n",
        "keep_prob = [1.0,1.0,1.0] #To set dropout probabilities for each layer (No dropout is used here)\n",
        "para,grd,ford,cst,test,train1 = model(X_train_final, Y_train_final, X_test_final, Y_test_final, layer, keep_prob, mini_batch_size=900, beta1=0.0, beta2=0.0, learning_rate = 0.5, iterations = 22000, lambd = 0.0 )"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 2.247486\n",
            "Cost after iteration 100: 1.916654\n",
            "Cost after iteration 200: 1.912270\n",
            "Cost after iteration 300: 1.909525\n",
            "Cost after iteration 400: 1.907662\n",
            "Cost after iteration 500: 1.906309\n",
            "Cost after iteration 600: 1.905277\n",
            "Cost after iteration 700: 1.904402\n",
            "Cost after iteration 800: 1.903682\n",
            "Cost after iteration 900: 1.903002\n",
            "Cost after iteration 1000: 1.902374\n",
            "Cost after iteration 1100: 1.901809\n",
            "Cost after iteration 1200: 1.901220\n",
            "Cost after iteration 1300: 1.900672\n",
            "Cost after iteration 1400: 1.900131\n",
            "Cost after iteration 1500: 1.899607\n",
            "Cost after iteration 1600: 1.899098\n",
            "Cost after iteration 1700: 1.898579\n",
            "Cost after iteration 1800: 1.898065\n",
            "Cost after iteration 1900: 1.897584\n",
            "Cost after iteration 2000: 1.897098\n",
            "Cost after iteration 2100: 1.896607\n",
            "Cost after iteration 2200: 1.896144\n",
            "Cost after iteration 2300: 1.895662\n",
            "Cost after iteration 2400: 1.895199\n",
            "Cost after iteration 2500: 1.894735\n",
            "Cost after iteration 2600: 1.894276\n",
            "Cost after iteration 2700: 1.893826\n",
            "Cost after iteration 2800: 1.893380\n",
            "Cost after iteration 2900: 1.892929\n",
            "Cost after iteration 3000: 1.892496\n",
            "Cost after iteration 3100: 1.892051\n",
            "Cost after iteration 3200: 1.891613\n",
            "Cost after iteration 3300: 1.891182\n",
            "Cost after iteration 3400: 1.890729\n",
            "Cost after iteration 3500: 1.890294\n",
            "Cost after iteration 3600: 1.889869\n",
            "Cost after iteration 3700: 1.889443\n",
            "Cost after iteration 3800: 1.889025\n",
            "Cost after iteration 3900: 1.888585\n",
            "Cost after iteration 4000: 1.888136\n",
            "Cost after iteration 4100: 1.887689\n",
            "Cost after iteration 4200: 1.887251\n",
            "Cost after iteration 4300: 1.886843\n",
            "Cost after iteration 4400: 1.886428\n",
            "Cost after iteration 4500: 1.886010\n",
            "Cost after iteration 4600: 1.885594\n",
            "Cost after iteration 4700: 1.885170\n",
            "Cost after iteration 4800: 1.884764\n",
            "Cost after iteration 4900: 1.884373\n",
            "Cost after iteration 5000: 1.883952\n",
            "Cost after iteration 5100: 1.883557\n",
            "Cost after iteration 5200: 1.883156\n",
            "Cost after iteration 5300: 1.882745\n",
            "Cost after iteration 5400: 1.882344\n",
            "Cost after iteration 5500: 1.881955\n",
            "Cost after iteration 5600: 1.881553\n",
            "Cost after iteration 5700: 1.881169\n",
            "Cost after iteration 5800: 1.880757\n",
            "Cost after iteration 5900: 1.880351\n",
            "Cost after iteration 6000: 1.879950\n",
            "Cost after iteration 6100: 1.879542\n",
            "Cost after iteration 6200: 1.879111\n",
            "Cost after iteration 6300: 1.878679\n",
            "Cost after iteration 6400: 1.878252\n",
            "Cost after iteration 6500: 1.877856\n",
            "Cost after iteration 6600: 1.877464\n",
            "Cost after iteration 6700: 1.877088\n",
            "Cost after iteration 6800: 1.876707\n",
            "Cost after iteration 6900: 1.876323\n",
            "Cost after iteration 7000: 1.875945\n",
            "Cost after iteration 7100: 1.875564\n",
            "Cost after iteration 7200: 1.875196\n",
            "Cost after iteration 7300: 1.874824\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 7400: 1.874458\n",
            "Cost after iteration 7500: 1.874094\n",
            "Cost after iteration 7600: 1.873726\n",
            "Cost after iteration 7700: 1.873355\n",
            "Cost after iteration 7800: 1.872997\n",
            "Cost after iteration 7900: 1.872625\n",
            "Cost after iteration 8000: 1.872269\n",
            "Cost after iteration 8100: 1.871901\n",
            "Cost after iteration 8200: 1.871543\n",
            "Cost after iteration 8300: 1.871189\n",
            "Cost after iteration 8400: 1.870848\n",
            "Cost after iteration 8500: 1.870484\n",
            "Cost after iteration 8600: 1.870127\n",
            "Cost after iteration 8700: 1.869773\n",
            "Cost after iteration 8800: 1.869413\n",
            "Cost after iteration 8900: 1.869051\n",
            "Cost after iteration 9000: 1.868708\n",
            "Cost after iteration 9100: 1.868367\n",
            "Cost after iteration 9200: 1.868028\n",
            "Cost after iteration 9300: 1.867684\n",
            "Cost after iteration 9400: 1.867343\n",
            "Cost after iteration 9500: 1.867004\n",
            "Cost after iteration 9600: 1.866672\n",
            "Cost after iteration 9700: 1.866331\n",
            "Cost after iteration 9800: 1.866006\n",
            "Cost after iteration 9900: 1.865673\n",
            "Cost after iteration 10000: 1.865335\n",
            "Cost after iteration 10100: 1.865018\n",
            "Cost after iteration 10200: 1.864683\n",
            "Cost after iteration 10300: 1.864362\n",
            "Cost after iteration 10400: 1.864029\n",
            "Cost after iteration 10500: 1.863702\n",
            "Cost after iteration 10600: 1.863355\n",
            "Cost after iteration 10700: 1.863016\n",
            "Cost after iteration 10800: 1.862677\n",
            "Cost after iteration 10900: 1.862355\n",
            "Cost after iteration 11000: 1.862015\n",
            "Cost after iteration 11100: 1.861640\n",
            "Cost after iteration 11200: 1.861163\n",
            "Cost after iteration 11300: 1.860889\n",
            "Cost after iteration 11400: 1.860480\n",
            "Cost after iteration 11500: 1.860160\n",
            "Cost after iteration 11600: 1.859807\n",
            "Cost after iteration 11700: 1.859516\n",
            "Cost after iteration 11800: 1.859152\n",
            "Cost after iteration 11900: 1.858809\n",
            "Cost after iteration 12000: 1.858467\n",
            "Cost after iteration 12100: 1.858141\n",
            "Cost after iteration 12200: 1.857727\n",
            "Cost after iteration 12300: 1.857396\n",
            "Cost after iteration 12400: 1.856996\n",
            "Cost after iteration 12500: 1.856621\n",
            "Cost after iteration 12600: 1.856285\n",
            "Cost after iteration 12700: 1.855873\n",
            "Cost after iteration 12800: 1.855507\n",
            "Cost after iteration 12900: 1.855083\n",
            "Cost after iteration 13000: 1.854734\n",
            "Cost after iteration 13100: 1.854372\n",
            "Cost after iteration 13200: 1.853991\n",
            "Cost after iteration 13300: 1.853583\n",
            "Cost after iteration 13400: 1.853239\n",
            "Cost after iteration 13500: 1.852834\n",
            "Cost after iteration 13600: 1.852455\n",
            "Cost after iteration 13700: 1.852026\n",
            "Cost after iteration 13800: 1.851628\n",
            "Cost after iteration 13900: 1.851232\n",
            "Cost after iteration 14000: 1.850805\n",
            "Cost after iteration 14100: 1.850448\n",
            "Cost after iteration 14200: 1.850085\n",
            "Cost after iteration 14300: 1.849705\n",
            "Cost after iteration 14400: 1.849340\n",
            "Cost after iteration 14500: 1.848954\n",
            "Cost after iteration 14600: 1.848600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 14700: 1.848292\n",
            "Cost after iteration 14800: 1.847910\n",
            "Cost after iteration 14900: 1.847407\n",
            "Cost after iteration 15000: 1.847008\n",
            "Cost after iteration 15100: 1.846652\n",
            "Cost after iteration 15200: 1.846263\n",
            "Cost after iteration 15300: 1.845882\n",
            "Cost after iteration 15400: 1.845497\n",
            "Cost after iteration 15500: 1.845103\n",
            "Cost after iteration 15600: 1.844674\n",
            "Cost after iteration 15700: 1.844317\n",
            "Cost after iteration 15800: 1.843895\n",
            "Cost after iteration 15900: 1.843454\n",
            "Cost after iteration 16000: 1.843046\n",
            "Cost after iteration 16100: 1.842659\n",
            "Cost after iteration 16200: 1.842228\n",
            "Cost after iteration 16300: 1.841719\n",
            "Cost after iteration 16400: 1.841391\n",
            "Cost after iteration 16500: 1.841033\n",
            "Cost after iteration 16600: 1.840743\n",
            "Cost after iteration 16700: 1.840360\n",
            "Cost after iteration 16800: 1.839973\n",
            "Cost after iteration 16900: 1.839653\n",
            "Cost after iteration 17000: 1.839312\n",
            "Cost after iteration 17100: 1.838983\n",
            "Cost after iteration 17200: 1.838612\n",
            "Cost after iteration 17300: 1.838291\n",
            "Cost after iteration 17400: 1.838018\n",
            "Cost after iteration 17500: 1.837609\n",
            "Cost after iteration 17600: 1.837336\n",
            "Cost after iteration 17700: 1.836965\n",
            "Cost after iteration 17800: 1.836597\n",
            "Cost after iteration 17900: 1.836235\n",
            "Cost after iteration 18000: 1.835975\n",
            "Cost after iteration 18100: 1.835624\n",
            "Cost after iteration 18200: 1.835236\n",
            "Cost after iteration 18300: 1.834940\n",
            "Cost after iteration 18400: 1.834602\n",
            "Cost after iteration 18500: 1.834289\n",
            "Cost after iteration 18600: 1.833970\n",
            "Cost after iteration 18700: 1.833690\n",
            "Cost after iteration 18800: 1.833273\n",
            "Cost after iteration 18900: 1.832962\n",
            "Cost after iteration 19000: 1.832678\n",
            "Cost after iteration 19100: 1.832305\n",
            "Cost after iteration 19200: 1.832014\n",
            "Cost after iteration 19300: 1.831656\n",
            "Cost after iteration 19400: 1.831371\n",
            "Cost after iteration 19500: 1.831036\n",
            "Cost after iteration 19600: 1.830655\n",
            "Cost after iteration 19700: 1.830368\n",
            "Cost after iteration 19800: 1.830188\n",
            "Cost after iteration 19900: 1.829764\n",
            "Cost after iteration 20000: 1.829496\n",
            "Cost after iteration 20100: 1.829150\n",
            "Cost after iteration 20200: 1.828779\n",
            "Cost after iteration 20300: 1.828541\n",
            "Cost after iteration 20400: 1.828206\n",
            "Cost after iteration 20500: 1.827923\n",
            "Cost after iteration 20600: 1.827607\n",
            "Cost after iteration 20700: 1.827402\n",
            "Cost after iteration 20800: 1.827032\n",
            "Cost after iteration 20900: 1.826776\n",
            "Cost after iteration 21000: 1.826494\n",
            "Cost after iteration 21100: 1.826218\n",
            "Cost after iteration 21200: 1.825904\n",
            "Cost after iteration 21300: 1.825621\n",
            "Cost after iteration 21400: 1.825316\n",
            "Cost after iteration 21500: 1.824999\n",
            "Cost after iteration 21600: 1.824730\n",
            "Cost after iteration 21700: 1.824410\n",
            "Cost after iteration 21800: 1.824238\n",
            "Cost after iteration 21900: 1.823886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HyrN6mJqqGCM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "74554486-4d02-47e5-9286-0ec13bd19b70"
      },
      "cell_type": "code",
      "source": [
        "print(test)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4OUw77pXqFyd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "69478578-04dc-497f-87fd-1558e5cb4717"
      },
      "cell_type": "code",
      "source": [
        "#for test set\n",
        "exam = np.zeros((1,100))\n",
        "exam1 = np.zeros((1,100))\n",
        "exam2 = np.zeros((1,100))\n",
        "exam3 = np.zeros((1,100))\n",
        "exam = train1 - Y_train2\n",
        "exam1 = test - Y_test2\n",
        "exam1[exam1!=0]=1\n",
        "exam[exam!=0]=1\n",
        "\n",
        "fp_train = np.sum(exam)\n",
        "fp_test = np.sum(exam1)\n",
        "\n",
        "corr_train = 900-fp_train\n",
        "corr_test = 100-fp_test\n",
        "accu_train = (corr_train/900)*100\n",
        "accu_test = (corr_test/100)*100\n",
        "\n",
        "print(\"true positive for train= \"+ str(corr_train))\n",
        "print(\"true positive for test= \"+ str(corr_test))\n",
        "print(\"Training Set Accuracy = \"+ str(accu_train)+\"%\")\n",
        "print(\"Test Set Accuracy = \"+ str(accu_test)+\"%\")\n",
        "\n"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "true positive for train= 483\n",
            "true positive for test= 53\n",
            "Training Set Accuracy = 53.666666666666664%\n",
            "Test Set Accuracy = 53.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7w3xqz8VqFB6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "59eefc76-d20c-4a66-b88c-6471c77d8bef"
      },
      "cell_type": "code",
      "source": [
        "costs = np.squeeze(cst)\n",
        "plt.plot(costs)\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(0.01)+\"Without ADAM\")\n",
        "plt.show()"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XmYZFV9//F39Vq9LzM9MzDs2xcU\nN9SYUQkMIKDBEBTEKBEixB8xGhM1JipJ3CIGRFTUJLhkIvmxRxAUkC0jIKIQwADCFwIMzAww09P7\n9DZLV/44p7qrqqt7eoau7um6n9fz9NNV9966depMz/3UOefec1OZTAYREUmeivkugIiIzA8FgIhI\nQikAREQSSgEgIpJQCgARkYRSAIiIJFTVfBdASsfMMsDe7r5ujt/3FOCd7v7BuXzf+N6nAze7e/8s\n7a8G+A7we8B24J/d/ZtFtksB5wOnABngOnf/dM7644H/AL7p7l+Ky34M3Ofu5+ds9zTwXXf/Ss6y\nNcCfAn8MXOPuN+Z+TjNbBfxvdr+z8JkNWOrud02zzVeBDwKvcfe1OctXA4cA/UADsB74lrv/R8Hr\nq4BHgefd/fic5fsBzwIXufsnC15zO3CQu+/3cj6fTFALQGadu183Hwf/6PNA8yzu7+NAO3Ao8Cbg\nL83sDUW2Ox04Gnh1/DnazE4FMLP3Af8APFjwmtuAY7JPzGzfWPaVOcsOBJYCd7v7B9z9xrhqtj9n\nrlMIgVdUPHifBFwInFFkk0+5+6Huvjfw58DfmNmnC7Y5EbgTWGJmywvWbQT+0MzGj09mthQ4cKc/\niUxLLYAEMrNawn/eE4Ea4FJ3/3JctwL4FuHb2xjwF+5+e/xmdi9wFXCEux8VWxgfIBwklwEXuPvF\nZnYWcIa7Hxe/nT4HvJnwzfBJ4GR3HzKzE4DvAZuBi4GvAq929zUF5V0D/AB4P/A2oA74PrAIqAb+\nzt2vMLMfAAasjmV4FLiEcOCuAr7o7v+2k9V1GvBZdx8D+s3s2rjsgSLbrXL30Vjmy+Kya4EnCAf1\nSwtecxtwgZnVxtcdA1wJnGZm1e6+NS67x91H4rfr78VluZ8ToN3MbgIOBx4HTnX3ATN7NfDPsa5G\ngL9x95/l/hvF8p5FOJh/A/g0sMXM2tz9E0Xq5ATgV8APgZ8RWj5Fuft/m9m7gIfM7Dvu3hdXnUlo\nWa2N7/tPOS8bBp4GjgR+Hpe9B7gDOG6q95KdpxZAMn0KeAXwKuCVwKlmdlJcdylwobsfCnwF+Jec\n1y0GHnb3o3KWvdLdXwf8AfBlM6ss8n6nEb4hHwh0AKfE7f4d+JC7HwYcTAidqezl7ubuzxOC4ifx\ndR8Evh8PmNlWx9Hufg9wESHEst/eP29mhxfu2MzuNrMnCn5+GVcfQjgYZT0d91doyu3c/UF331L4\nAnd3oBNYERcdA9wNPAL8Ts6y2wpeV/g5AY4nHEgPAJYw8Q36SkIXzKHAOcAVZtZUpPzZfd8IXAd8\nY4qDP8BZwGXuvh7YYGZvnGp/cZ9PAc8DvwtgZu3Aa4HVwOUUb0VcDbwv5/l7gWumex/ZeWoBJNM7\nga/Eb52jZvZD4F3ATwj/MbPzg9xNOKBkVRMODrkui78fBNKEg0+hn7p7N4CZPQLsQzhg1rr7zXGb\nS4BPFnlt1k9yHp8MpOLje+L77kE4yBR+zhPjt/dOM/tR/JyP5m7k7kdO8771hG/OWcMUD6qZblfo\nNuBYwsHwaOAThLpZCfwi/r5gBvu5KaeOHwX2AvYntMyuBHD3B8zsOWDaA/Z0zKwNeD2h+wbCuMYH\ngPt38NJ+oCU+fi/wn+6eAZ4zs24ze727/3fO9j8CvmRmHwH2JNSv72q5pTi1AJKpFbg4+20X+BgT\nB6v3A782MyccnFI5r9teZHC1D8Ddt8fnxVoAfTmPt8dt2oCenOUv7KDM3TmPTwDuMrMngd/GMhb7\nW24Frs75nKew8/3mg4SAyaondFnt6naFbgOONbODgX5330gIg5Vm9grC53p4BvvJ/XfJ1nEH0BsP\ntFk9FA/pmfojwgG528x6ga8Dp5tZ9Q5etx+hbx9CC+LDZtYb9/FGQpfQOHfvIYTKCYTW49Uvo8wy\nBbUAkukF4KvunvutmjgY913gTe7+cDwoPVmiMvQDjTnPl83kRfFAcw3wHne/KY5nDE+x+QvAH7r7\no1Osz+7zbsLBMlePu68g9N8fBDwVlx9MCJ1C2e1u28F2hW4HVhHGNlbHZb8GXkPo/rmj4AC+MzYQ\nxgZSOftYFJfvRX5Yt81wn2cSup7uyy6IZzP9PnB9sReY2VsJ4fhrMzsMaHb35pz1i4FHzKywy+lK\n4FRCN+XpMyyf7AS1AJLpx8A5ZlZpZikzO8/MTiQcBAeBJ+KZHh8CMLPGafa1q54Cqs3s6Pj8XCa6\nnqbTEH+yg7AfA7YwESbbCN/8IXzOcyGcuWJmF5vZEYU7dPcj41kruT/ZfvmrgY/GutqD0H1xVZFy\nXQ18yMwaYn19CLhiRx/G3buAx2I5/ysu20L41n8OBf3/OXI/51TWAOuIB08zezMhaH8NvBgWWdrM\n6gkH2qytxfYdD957EwaAc11P6AaaxMxeQxjAP8/dhwjf/vOCwt03Eb5ovL3g5T8mdIttd/dnpv2k\nsksUAOVvdcHg5luBbxPOzHmM8M31MEJf+m+Amwj/GX8J3Ajcx8SZGLMmjj/8GbDKzB6O7znGDkLA\n3XsJfeIPmdlDhMHW64GfmFkD4UB8r5m9B/g7oCV2Zz1G+Mb7PztZ1G8QWhJOOEB/wd1/A2Bm55vZ\nubFc1wK3EA7cDxL6uG+M2/0gpwvqY/Hf4SM573Eb4eyd3HpeTWgF3D5FuXI/Z1HxW/97gY+Y2ePA\nN4HT3H0wfpZfEer9ZsLBNutG4Nx4xlOuM4EbirRIbgROiIO7EM5sesLMnieMEX3R3b8VB/7PoHhL\n4ToKQiSW8z40+FsyKd0PQHYH8eC9GWjNOVVQREpILQCZN2Z2v4UrWiF0Uzyug7/I3NEgsMynvwK+\nbWZfJAwKn7mD7UVkFqkLSEQkodQFJCKSUAumC6izc2CXmyptbfX09AzNZnHKguplMtXJZKqTyRZS\nnXR0NKWmWpeIFkBVVbGLU0X1MpnqZDLVyWTlUieJCAAREZlMASAiklAKABGRhFIAiIgklAJARCSh\nFAAiIgmlABARSaiyD4CuvhH+/ae/ZXTL9h1vLCKSIGUfAA/4Rq698ymeXNc730UREdmtlH0AjMXJ\n7sbGNOmdiEiusg8AEREpLjEBoO//IiL5yj4AUkw5EZ6ISKKVfQCIiEhxCgARkYRKTgBoEEBEJE9y\nAkBERPIoAEREEkoBICKSUIkJgIwGAURE8pR9AKR0GYCISFFlHwAiIlKcAkBEJKGSEwAaAhARyVP2\nAaAhABGR4so+AEREpLjEBIB6gERE8lWVcudmdgFwZHyf8939RznrVgLnA9sBB85x97FZL4TOAxUR\nKapkLYB4gD/c3VcAJwJfL9jkUuBUd38L0BS3ERGROVLKLqC7gNPi416gwcwqc9a/3t3XxcedwKIS\nlkVERAqUrAvI3bcDg/Hp2cBNcVl2fT+Ame0BHA/83XT7a2urp6qqcrpNimpsrAWguTlNR0fTTr++\n3KlOJlOdTKY6mawc6qSkYwAAZnYyIQCOL7JuCXAj8GF375puPz09Q7v0/oObRwHo6xuhs3Ngl/ZR\nrjo6mlQnBVQnk6lOJltIdTJdUJV6EPgE4LPAie7eV7CuGbgZ+Ky731rKcoiIyGQlCwAzawEuBI5z\n9+4im1wEXOzut5SqDCIiMrVStgBOBxYDV5tZdtmdwCPAz4APAAeb2Tlx3eXufmnpiqMrAUREcpVy\nEPhSwqmeU6kt1Xvn0WUAIiJFJeZKYBERyacAEBFJqMQEQEZDACIieco+ADQEICJSXNkHgIiIFKcA\nEBFJqLIPgJSmgxYRKarsA0BERIpTAIiIJFRiAkBngYqI5EtMAIiISD4FgIhIQikAREQSKjEBkNFc\nECIieco+AHQZgIhIcWUfACIiUpwCQEQkoco+ANQDJCJSXNkHgIiIFKcAEBFJqMQEgM4CFRHJV/4B\noPNARUSKKv8AEBGRohQAIiIJlZgAyGhCaBGRPGUfABoBEBEpruwDQEREilMAiIgkVHICQEMAIiJ5\nyj8ANAggIlJU+QeAiIgUlZgAUA+QiEi+sg8A9QCJiBRX9gEgIiLFKQBERBIqOQGgQQARkTxlHwAp\nTQctIlJU2QeAiIgUV1XKnZvZBcCR8X3Od/cf5axLA/8KvNLd31DKcoiIyGQlawGY2UrgcHdfAZwI\nfL1gkwuBh0v1/oU0HbSISL5SdgHdBZwWH/cCDWZWmbP+M8B1JXx/ERGZRsm6gNx9OzAYn54N3BSX\nZdcPmNmime6vra2eqqrKHW9YoKkpHX/X0dHRtNOvL3eqk8lUJ5OpTiYrhzop6RgAgJmdTAiA41/O\nfnp6hnbpdQMDIwD09w/T2TnwcopQdjo6mlQnBVQnk6lOJltIdTJdUJV6EPgE4LPAie7eV8r3mopO\nAhURKa5kAWBmLYSB3uPcvbtU7yMiIrumlC2A04HFwNVmll12J/CIu19nZtcAewNmZquBS9398hKW\nR0REcpRyEPhS4NJp1p821ToRESm98r8SWIMAIiJFlX8AiIhIUQoAEZGESkwAZDQThIhInrIPgJQG\nAUREiir7ABARkeIUACIiCZWYANB00CIi+co+AHRHSBGR4so+AEREpLjkBIB6gERE8iQnAEREJI8C\nQEQkoRQAIiIJlZgA0BCAiEi+sg8AnQYqIlLcjALAzFqLLNt/9osjIiJzZYd3BDOzCuA6MzuGidur\nVAM3AK8qYdlERKSEpm0BmNkfAU8ARwHbgK3x9xDwfMlLJyIiJTNtC8DdrwCuMLPPufvn5qZIs0vT\nQYuIFDfTQeBVZvYWADP7UzP7vpkdVsJyiYhIic00AP4N2GJmrwP+FPhP4JslK5WIiJTcTAMg4+73\nA6cAl7j7TbCw+lYyuiekiEieHZ4FFDWa2RuBU4GjzKwWaCtdsWbRgoopEZG5M9MWwEXAd4F/dfdO\n4HPA5aUqlIiIlN6MWgDufhVwlZm1m1kb8Bl3X1B9KguqsCIic2CmVwK/xcyeJlwT8BTwuJm9oaQl\nmyXqARIRKW6mXUDnAye7+xJ3Xwz8EfC10hVLRERKbaYBsN3dH80+cfeHCFcEi4jIAjXTs4DGzOzd\nwG3x+YnA9tIUqUQ0CCAikmemAXAucAnwPWAMeJhwQdjuT4MAIiJFzbQL6Hhg1N3b3H0R4bD6jtIV\nS0RESm2mAXAG8K6c58cD75v94oiIyFyZaQBUuntun3+GBda5oiEAEZF8Mx0DuMHM7gXuJoTGsYQJ\n4XZ7mg5aRKS4GbUA3P1LwKeAjcCLwIfd/R9LWTARESmtmbYAcPd7gHtKWJbS0mygIiJ5ZhwAu8LM\nLgCOjO9zvrv/KGfdccCXCdcT3OTuXyxFGVLqARIRKWqmg8A7zcxWAoe7+wrChWNfL9jkm8C7gbcA\nx5vZK0pVFhERmaxkAQDcBZwWH/cCDWZWCWBmBwDd7r7W3ceAmwgDyyIiMkdK1gUUTxsdjE/PJnTz\nZE8lXQZ05my+ETiwVGUBnQYqIlKopGMAAGZ2MiEAjp9msx321Le11VNVVbnT79+8vh+AxsY0HR1N\nO/36cqc6mUx1MpnqZLJyqJNSDwKfAHwWONHd+3JWvUBoBWQtj8um1NMztEtl6O8fBmDz5hE6Owd2\naR/lqqOjSXVSQHUymepksoVUJ9MFVSkHgVuAC4GT3L07d527rwGazWw/M6sCTgJuLVVZRERkslK2\nAE4HFgNXm1l22Z3AI+5+HfBnwBVx+VXu/mQJy6LLAERECpRyEPhS4NJp1t8FrCjV+2eldCGAiEhR\npTwNVEREdmMKABGRhFIAiIgkVNkHgEYARESKK/sAEBGR4hITABmdByoikicxASAiIvkUACIiCaUA\nEBFJqMQEgEYARETylX0AjE8FoQQQEclT9gFQkT3+6ywgEZE8ZR8A2RbAmI7/IiJ5EhAA4XdGfUAi\nInmSEwA6/ouI5ElAAIQE0BiAiEi+BARA+K3jv4hIvgQEgFoAIiLFlH0AZD+gjv8iIvnKPgAmTgNV\nAoiI5EpAAITfOv6LiORLQADEMYB5LoeIyO4mAQEQfmsQWEQkXwICIHsW0DwXRERkN5OAAAi/1QIQ\nEclX/gGAWgAiIsWUfwDEFoBOAxURyVf2AVCRTQAREclT9gGgFoCISHFlHwDVVeEjbtm6fZ5LIiKy\neyn7AGhvSgOwqW9knksiIrJ7KfsAqK2ppL05zdqNm9k+NjbfxRER2W2UfQAArHjVHgwMbeV/nu6a\n76KIiOw2EhEAJ67YjxRw3V3PMqa7w4uIAAkJgP32aGbF4ctY17mZn/5yzXwXR0Rkt5CIAAA4/ZiD\nWNRcy/V3P8vqh9fPd3FEROZdYgKgqb6Gj7771TTUVfPDW5wf3PQ4m4e3znexRETmTWICAGCfpU18\n+owj2GdJI/f8z4v89Xfu5Yrbn+K5lwY0WZyIJE5VKXduZocDPwYudvdvFaw7GTgPGAWuLFxfKnss\nauC8M9/Afz24nlt+/Ty3PbCW2x5YS0drmlfuv4jD9m3joOUttDXVzkVxRETmTckCwMwagEuAO4qs\nqwC+BRwBdAE3m9n17r6uVOXJVVVZwdveuDdHv245jzzTxa9+u4FHn+1i9UPrWf1QGB9ob65l36VN\n7Lusafx3a6NCQUTKRylbAKPAO4C/KbJuMdDr7p0AZnYHcBywqoTlmaS6qoIjDungiEM62D42xpoX\nB3ji+R6eXt/PMy/08dBTm3joqU3j2zfVV7NXR2P4WdLA3ksa2WNRA7XVlXNZbBGRWZEqdd+3mX0O\n2JTbxWNmKeBZ4G3AGuAGYLW7/9NU+9m2bXumqmruDrSZTIbu/hGeXt/HM+v7eHpdL2te7OelrqG8\n7VIpWLaogX2XNbH30vizpIm9ljSSri1pD5uIyExMOSXyvByh3D1jZmcCPwD6CGEw7bzNPT1D062e\nVkdHE52dA7v02v07Gti/o4FjX7snAMOj21i/aZB1GzezrnMz6zsHWde5mfseHeS+R1/Ke+2i5lr2\nWNQQfhbXs+eiBpYtqqeprnr8VpXz6eXUS7lSnUymOplsIdVJR0fTlOvm7Suqu/8cOBLAzM4ntAR2\ne3W1VRy0vIWDlreML8tkMvQPbuGFriFe7BrkhU2DvNg1xAtdgzz6bDePPtudt4+GdBXL2uvDz6J6\nlrU3sKy9jiVtdVTPYStHRJJt3gLAzG4GzgQGgXcCF81XWV6uVCpFS2MtLY21HLZvW966oZGt42Hw\nYtcQL3UN8VL3EGteGuDpF/rz9wO0N6dZ1l7H0mxAtNezpL2eRc21VFYk6qxdESmxUp4F9HrCQX0/\nYKuZnUro63/W3a8DvgvcCmSA891901T7Wsjq09UcuLyFA3NaDADbto+xqW+El7qGeLF7kA3dw2zo\nHmJDzxCPrenhsTU9edtXVqRY3FrH0rY6lrbVszSGxNK2Otqb07rzmYjstJIPAs+Wzs6BXS7oQuqv\nAxjZso0N3cO81B1aCxt7htjYM8yGnuGiVy9XV1WwpHUiEDra6ljSGn7am9NUVBQPh4VWL3NBdTKZ\n6mSyhVQnHR1Nu9cgsEwvXVMVrj9YNnnwZnBka2gt9AzFFkMIig3dQ6zfNDhp+8qKFItb0hOh0FbP\nktYQEs2t9XPxcURkN6UAWGAa0tUcsGc1B+zZnLc8k8nQP7SVDd2htbCxd5jO3mE29oTfG57pLrq/\ntqZaOmJrYSIk6uhoraOxrnouPpKIzBMFQJlIpVK0NNTQ0lDDIXu3Tlo/NLItBELvMBt7hujsHaZ3\ncCvrNw7w1NpenlzbO+k19bVVk0JhSWv43dZUO2XXkogsDAqAhKhPT+5WyvZjbt02xqa+iRbDxt5h\nOuPv9Z2DPPfS5L7O8a6lGAjhZ+J5nS6CE9nt6X+pUF1VMX7BWqGxTIbegdHxrqTOvmE6e0fC497h\nSdc4ZDXWVU8Khezz9qapB6ZFZO4oAGRaFakU7c1p2pvTHFpwjQOEK6M39U0EQvgJz9duHODZF/sn\nvaayIsWi5jSLWsLP4vizqDnN4pY6WptqdM2DyBxQAMjLUldbxd5LGtl7SeOkddnWQ3bsobN3hE3x\n8aa+ER5/rqfIHrOhU8uiGDyLcgJiUUuaRc21umJaZBYoAKRkclsPts/k1sOWrdvp6h+hq2+ETdnf\nfdnfwzy5tpepLv5orq8e33d7cy3tTSEc2ptqaW9O09JQo24mkR1QAMi8qamunHLsAWDrtjF6BkIg\ndPWPjodFV3/4Wdc5yJoiA9QQupnammpDILSEcYdFzbW0Nadjy6KW+tqq3WJSPpH5ogCQ3VZ1VUW4\ncK2t+AVrmUyGgaGtdA+M0NU3SvfACN39I3T3j4bfA6M8tb6PzLq+oq+vra4MrYfmEA7tTWn2Xd5K\ndSpDe3OatqZa3etBypoCQBasVCpFc0MNzQ017Les+Dbbto/Ru3k0LxS6+kfoiS2K7v4RXuyaeqrx\nhnQVbU3pvKCYGItI09qo6yFk4VIASFmrqqxgcUsdi1vqptxmdMv20IroH2FrJsVz63vp7h+lZyAE\nRmfvMOs6Nxd97XhXU3P+IHU2INqb02pFyG5LASCJV1szMRYRLo5rz1ufyWQYHt2WNw7R3T8xFtHV\nNxKupp5i/4111bQ21k7qbmprqmVJW51aETJvFAAiO5BKpahPV1Ofri56uiuErqbugVG6+/KDoat/\nhJ6BUTr7dtyKWNwy0bXU0Vo3fqW1AkJKRQEgMguqKivGp+CeytDItvGWQ/dAGJPo7B2mqz+c/vrE\n85PnY4KJC+cWt6Zjd1Z6PCwWt9TR0lij+0HILlEAiMyR+nQV9elG9pqiFbF12xjdAyEMNmUvnOsb\nHn/+2zU9wOSL56qrKmIo1MWQSLOkNd4borWO2hqNQUhxCgCR3UR1VUW429sUp72ObtnOpv4QBt2x\n1dAZp+HY1Ds85dlMzQ01dLSkWZzTrbQ4Pm9vqqWqUtNuJJUCQGSBqK2pZPniBpYvLn7h3NDINjb1\nDdPVN8KGnompvzf1jhS9BzVMTLuRDYRsUHTEx80NNaX+WDKPFAAiZaI+XcU+6Sb2WTr5TnJjY5kw\nGB1ndN0Uu5eyLYgnnu+FImMQNVUVLF1UT1tjbU7rIczquriljvq0DiELmf71RBKgoiI1PvvqoUw9\nL1N23CF0K43Q2TdMV/8oazcUP4OpIV01PvbQ0VrHsvZ6lrXXs6StjpaGGk21sZtTAIjItPMydXQ0\n8dza7vFpvsPYw0QrYv2mQZ7bMHlOpprqiry7yC2Jd5dra07T0ZKmRhfIzTsFgIjsUH26mn2XVefd\nUS5rLJOhb/MWOnuHeal7iA094b7UuXeVK6YhXUV7zjUP2ZlcW+N9qpvrq9WCKDEFgIi8LBWpcCFb\nW1PtpPtRZzIZBoa30hnvKJe9F0R20r4NPUOs3Vi8e6m6qmJ8au/WxpqJ6TbilBttTbU0pDWj68uh\nABCRkkmlUjTX19BcX8OBy1smrc9kMvQPbR2fXqMnTtbXlXN/iA3dU0/WV11VQVtjLa1NYaqNMAX4\nxD0iWhtraKrXvSGmogAQkXmTSqVoaaihpaGGA/ZsLrrN2FiGvsEt47O3dvYO0z0wSu/AKD3x56lp\nbh5UVVkR5l9qDt1MbfF+EG2NITAWtaQTe28IBYCI7NYqKia6mCjSioD8ab97BrL3hhilb/MonbF1\nsWGKW5BCuMaiPb5H+JkIiYa6atqbamluqCm7i+YUACKy4M1k2u+t27bHLqYwD1Pv5hAWXX0jMTRG\np703BISrqlsaaljSXk9DbdVEaDSH0GhrrKFuAbUmFAAikgjVVZXT3mEOwvUQPeMtiRAMg8Pb6B4Y\noX9wC939o2zsHZ5y4BrCxXOtjbXjYdHenKalMYyDtDbV0NJQS0tjDU1183+WkwJARCSqqa6cdj6m\nrIamNP+7povugVF6cm4e1DswSu/mLfRsHqXzhT4yUw1MEGZ5bayrpq2pNoyDxHtGtMXwyIZIc0M1\nlRWl6XpSAIiI7KT6dPWUF85lZTKZ0GoYGKV/cAv9g1voGRild3ALfZvjsqEtrOscZM1Lky+ky6pI\npXjf2w7mmCP2mvXPoQAQESmBVCpFS2MtLY21026XveNcz+Ytodupf5T+oS30bQ4B0T+4hdYd7GNX\nKQBEROZR7h3npprptVTK65wmERGZMQWAiEhCKQBERBJKASAiklAKABGRhFIAiIgklAJARCShFAAi\nIgmVykw3WYWIiJQttQBERBJKASAiklAKABGRhFIAiIgklAJARCShFAAiIgmlABARSaiyvyGMmV0M\n/C6QAT7m7vfPc5HmjJkdDVwDPBYXPQJcAFwGVAIvAn/s7qNm9n7gL4Ex4FJ3//7cl7h0zOxw4MfA\nxe7+LTPbmxnWg5lVA6uAfYHtwJ+4+zPz8TlmW5F6WQW8HuiKm1zo7j9NUr2Y2QXAkYTj4/nA/ZTp\n30pZtwDM7CjgYHdfAZwNfHOeizQffu7uR8efjwJfAL7t7kcC/wt80MwagL8HjgOOBv7KzNrnrcSz\nLH6+S4A7chbvTD28D+h197cC/0g4KCx4U9QLwKdz/mZ+mqR6MbOVwOHxmHEi8HXK+G+lrAMAOBa4\nHsDdHwfazKx5fos0744GboiPbyT8Ab8JuN/d+9x9GPgF8Jb5KV5JjALvAF7IWXY0M6+HY4Hr4ra3\nUz51U6xeiklSvdwFnBYf9wINlPHfSrkHwDKgM+d5Z1yWJK8wsxvM7B4zexvQ4O6jcd1GYA8m11N2\neVlw923xP2munamH8eXuPgZkzKymtKUuvSnqBeAjZnanmV1pZotJUL24+3Z3H4xPzwZuooz/Vso9\nAAql5rsAc+wp4PPAycCZwPfJH/eZqj6SVk87Ww/lXD+XAX/r7scADwOfK7JN2deLmZ1MCICPFKwq\nq7+Vcg+AF8j/xr8nYRAnEdx9vbtf5e4Zd38aeInQDVYXN1lOqKPCesouL2ebd6IexpfHQb6Uu2+Z\nw7LOGXe/w90fjk9vAF5FwuqPDgwtAAAGKUlEQVTFzE4APgu83d37KOO/lXIPgFuBUwHM7AjgBXcf\nmN8izR0ze7+ZfTI+XgYsBf4NeHfc5N3ALcCvgDeaWauZNRL6Le+ehyLPpduZeT3cykS/8DuB/5rj\nss4ZM/tPMzsgPj0aeJQE1YuZtQAXAie5e3dcXLZ/K2U/HbSZfQX4PcKpWn/u7r+Z5yLNGTNrAi4H\nWoEaQnfQQ8APgTTwHOE0ta1mdirw14TTZS9x9/8/P6WefWb2euAiYD9gK7AeeD/hdL0d1oOZVQLf\nAw4mDJye5e5r5/pzzLYp6uUS4G+BIWAzoV42JqVezOxDhG6vJ3MWn0n4nGX3t1L2ASAiIsWVexeQ\niIhMQQEgIpJQCgARkYRSAIiIJJQCQEQkoRQAMuvM7LVmdkl8/Ip4DcZs7HdPMzsmPj7LzM6ejf1O\n8V6VZnaTma2Y5f2uMrNzZnOfcb//YWZnleL18Vz3e81s+a7uX3ZPZT8dtMy9eCXpR+PTU4ANwIOz\nsOuVwGHAne6+ahb2N52PA79x91+W+H12e+7ea2afI5zf/vZ5Lo7MIl0HILMu3ofgS4SLZK4D+ggX\nod0M/AvQAbQAF7n75fHgsj9hDvVPAHXAPxEupKkHPgz0EK6qTAHfAJqBKnc/z8x+nzA171D8+ZC7\nrzezNXHbt8f9n+vud5jZx4AzcrY/w92z899jZlWES/oPjxdBrQKGgQMIE36tcvevxUm+vg0cBDQB\nV7j7RfGb9ElAG/A1d/9pzr5XAf2Ei68Oifv6SqyDKnc/L263hjDr5Fvj70rAgDWEq1FThLmdXkW4\nOKkBuBJYTZix8hHgUXf/spl9mXClah3wc+BT07z+WsLFg21ANXCju/9jLNPDhAubslNFyAKnLiAp\nmfjt+RbCTUUuJ4TCLXGisd8DvmBmHXHz/YGV7v7fwGLgz+J23wA+4+7PEq7cvczdv5Z9DzOrJ3wz\nfbe7rySEzJdyijHs7sfHZX8Rl32BcKn/UYT53vcsKPobgefcfWPOsuXufkIs93lmtgj4GGF6kZWE\n6YHfa2avjtu/FnhH7sE/xxJ3/wPCgf2z09Vh9Gbgg4Qbtbwm7vs44NBY1j+Oy7MOAz4fD/6nxbIf\n5e6/Qwirk6Z5/duA6jj3/ZsJ8+BkjxO3EebIlzKhLiCZSysJ86ecGZ9vJRz4Ae5z92xz9CXgq2aW\nJrQUeqbZ5yHABndfF5+vBs7NWb86/n4OyN7k5vvALWZ2LXCNu+de9g+wN1B4+f6tMN4d8iThUv+V\nwF7xxkMQpgo4KD5+MGcK4UKr477WmVljnD5gOr/OTttsZmvj53gVcG+ssyEz+1XO9t3u7vHxSmCF\nma2Oz1sIdV49xet/QQjmqwlTIX8vTmsMoQ4P30FZZQFRAMhcGgU+7O4P5C40s3cAuTMmXgb8P3e/\n08xOAj45zT4L+zBTBcu2FazD3T9uZvsSboZyvZl9wt1v3kHZc1vL2fcYBb7g7tcWfJ6zCj5PoW0F\nzwvLDGHupum2TxHmt8rKDZHc9x4l3K7wqwVl/GSx18cur9cAKwjTiD9gZkdMcd8AWeDUBSSlNkb4\ntglwD/AeADOrM7PvxP72QkuBx+I349OA2iL7ynoSWGJm+8TnxwH3TVUYM2uL/e1r3f2fCX34v1Ow\n2VpCKyDXyuzrCd/yveDzVJjZ117GrTT7s+9pZq8Eluxg+98Cv2tmqTjp35um2O4e4F3Zejazvzez\ng6d6vZkdD/y+u//C3T9FmBAuW5Z9CWMQUiYUAFJqdwL/YGYfJsyyeLCZ3UO49d5D7l747RbCAPCd\nhMHMVcDeZvaXhOl2/8TMvpjdMH4zPRu4KnZzHAucN1Vh3L2HMGB7v5ndTugP/27BZvcD++SMTwD0\nmNn1hEHUf3D3XkJ4bDazXxJCpzdnCuGddQ3wOjO7GzgHeGwH2/8MeJ4wLfEPgKnOVvoRoVvn3ljO\npcAz07zegU+Y2d2xPm919+fiuuMIYzpSJnQWkEgRZvbXQJu7fyaeuXOPu39vnos1byzcTvTj7q7T\nQMuIWgAixX0NeO1sXwi2EJlZK+E03lm/gE3ml1oAIiIJpRaAiEhCKQBERBJKASAiklAKABGRhFIA\niIgk1P8BV67U17mR/wQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd347709e10>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "RVneVCf1r8yx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Results Observed\n",
        "\n",
        "** The training of the model takes too many iterations and the accuracy value obtained is really low**"
      ]
    },
    {
      "metadata": {
        "id": "6PCVNa8vr9HM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Scenario 2\n",
        "\n",
        "**Here the Parameters are optimised in a trial and error format to obtain the maximum accuracy, Here The network has 3 hidden layers with 40,25,and 15 neurons respectively and Droputs is used along with mini-batch Gradient Descent while Adam's Optimization is used to Minimise the cost fluctuation in Mini-Batch Gradient Descent**"
      ]
    },
    {
      "metadata": {
        "id": "qeowsx09rnQq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "7a2fada4-b179-4a66-fef0-e5ec2dac6c58"
      },
      "cell_type": "code",
      "source": [
        "layer = [10,40,25,15,4] #to set number of layers and the number of nodes in hidden layers\n",
        "keep_prob = [1.0,1.0,0.8,0.7,1.0] #To set dropout probabilities for each layer\n",
        "para,grd,ford,cst,test,train1 = model(X_train_final, Y_train_final, X_test_final, Y_test_final, layer, keep_prob, mini_batch_size=64, beta1=0.9, beta2=0.99, learning_rate = 0.01, iterations = 2300, lambd = 0.0 ) "
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 2.239360\n",
            "Cost after iteration 100: 2.782453\n",
            "Cost after iteration 200: 2.011477\n",
            "Cost after iteration 300: 0.853013\n",
            "Cost after iteration 400: 0.420394\n",
            "Cost after iteration 500: 1.202766\n",
            "Cost after iteration 600: 0.634106\n",
            "Cost after iteration 700: 1.485248\n",
            "Cost after iteration 800: 0.090717\n",
            "Cost after iteration 900: 0.007900\n",
            "Cost after iteration 1000: 0.063177\n",
            "Cost after iteration 1100: 0.003229\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in multiply\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 1200: 1.392841\n",
            "Cost after iteration 1300: 0.194062\n",
            "Cost after iteration 1400: 0.211843\n",
            "Cost after iteration 1500: 0.068209\n",
            "Cost after iteration 1600: 0.559016\n",
            "Cost after iteration 1700: 1.692567\n",
            "Cost after iteration 1800: 0.164614\n",
            "Cost after iteration 1900: 0.002012\n",
            "Cost after iteration 2000: 0.006288\n",
            "Cost after iteration 2100: 0.570533\n",
            "Cost after iteration 2200: 0.000050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lEKN0M1B5q1F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e06b16d8-f706-498e-f596-29ba27302754"
      },
      "cell_type": "code",
      "source": [
        "print(test)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0\n",
            " 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 1 1 2 0 1 0 0\n",
            " 3 0 0 1 0 2 1 0 0 1 2 0 1 0 0 3 0 0 1 0 2 1 0 0 1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fibc4kTOSS09",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Finding the distribution of correct positive and negative predictions for the Validation set**"
      ]
    },
    {
      "metadata": {
        "id": "1XoWDR6YFePa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0e6cd0e0-1dea-4eec-c59c-8519b67f9edc"
      },
      "cell_type": "code",
      "source": [
        "#for test set\n",
        "exam = np.zeros((1,100))\n",
        "exam1 = np.zeros((1,100))\n",
        "exam2 = np.zeros((1,100))\n",
        "exam3 = np.zeros((1,100))\n",
        "exam = train1 - Y_train2\n",
        "exam1 = test - Y_test2\n",
        "exam1[exam1!=0]=1\n",
        "exam[exam!=0]=1\n",
        "\n",
        "fp_train = np.sum(exam)\n",
        "fp_test = np.sum(exam1)\n",
        "\n",
        "corr_train = 900-fp_train\n",
        "corr_test = 100-fp_test\n",
        "accu_train = (corr_train/900)*100\n",
        "accu_test = (corr_test/100)*100\n",
        "\n",
        "print(\"true positive for train= \"+ str(corr_train))\n",
        "print(\"true positive for test= \"+ str(corr_test))\n",
        "print(\"Training Set Accuracy = \"+ str(accu_train)+\"%\")\n",
        "print(\"Test Set Accuracy = \"+ str(accu_test)+\"%\")\n",
        "\n"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "true positive for train= 900\n",
            "true positive for test= 99\n",
            "Training Set Accuracy = 100.0%\n",
            "Test Set Accuracy = 99.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g7Ib13mRR6BG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Graph for the reducing loss of the training set**"
      ]
    },
    {
      "metadata": {
        "id": "hBu6gKuvNxSG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "823ab126-d172-485e-8987-f2114c4b81d0"
      },
      "cell_type": "code",
      "source": [
        "costs = np.squeeze(cst)\n",
        "plt.plot(costs)\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(0.01)+\" , With Mini-Batch and ADAM\")\n",
        "plt.show()"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd347912208>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9jIVGowXtFBz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Results Observed\n",
        "\n",
        "** This combination of parameter values gave a really high validation accuracy of nearly 100% with a very less number of epochs of 2300 count**"
      ]
    },
    {
      "metadata": {
        "id": "yypIMl7e0cDx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Final Results Displayed Below"
      ]
    },
    {
      "metadata": {
        "id": "Rg2V_Tw4n9Ft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2292
        },
        "outputId": "bc8daf27-caba-4cca-f048-e3263409661a"
      },
      "cell_type": "code",
      "source": [
        "u = np.zeros((100,3))\n",
        "u[:,0]=arr[:100,0]\n",
        "u[:,1]=test[:]\n",
        "\n",
        "columns=['Input','Prediction','Output']\n",
        "\n",
        "import pandas as pd\n",
        "u1 = pd.DataFrame(u,columns=columns)\n",
        "\n",
        "for i in range(100):\n",
        "  if u[i][1]==0:\n",
        "    u1['Output'][i] = u1['Input'][i]\n",
        "  elif u[i][1]==1:\n",
        "    u1['Output'][i] = 'Fizz'\n",
        "  elif u[i][1]==2:\n",
        "    u1['Output'][i] = 'Buzz'\n",
        "  elif u[i][1]==3:\n",
        "    u1['Output'][i] = 'Fizz-Buzz'\n",
        "u1\n"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input</th>\n",
              "      <th>Prediction</th>\n",
              "      <th>Output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fizz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Buzz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fizz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fizz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Buzz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fizz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Fizz-Buzz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fizz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Buzz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>21.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fizz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>22.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>24.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fizz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>25.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Buzz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>26.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>27.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fizz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>29.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>30.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Fizz-Buzz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>71.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>72.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fizz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>73.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>74.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>75.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Fizz-Buzz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>76.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>77.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>78.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fizz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>79.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>80.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Buzz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>81.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fizz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>82.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>83.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>84.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fizz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>85.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Buzz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>86.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>87.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fizz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>88.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>89.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>90.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Fizz-Buzz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>91.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>92.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>93.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fizz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>95.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Buzz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>96.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fizz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>97.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>98.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>99.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fizz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>100.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Buzz</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows  3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Input  Prediction     Output\n",
              "0     1.0         0.0          1\n",
              "1     2.0         0.0          2\n",
              "2     3.0         1.0       Fizz\n",
              "3     4.0         0.0          4\n",
              "4     5.0         2.0       Buzz\n",
              "5     6.0         1.0       Fizz\n",
              "6     7.0         0.0          7\n",
              "7     8.0         0.0          8\n",
              "8     9.0         1.0       Fizz\n",
              "9    10.0         2.0       Buzz\n",
              "10   11.0         0.0         11\n",
              "11   12.0         1.0       Fizz\n",
              "12   13.0         0.0         13\n",
              "13   14.0         0.0         14\n",
              "14   15.0         3.0  Fizz-Buzz\n",
              "15   16.0         0.0         16\n",
              "16   17.0         0.0         17\n",
              "17   18.0         1.0       Fizz\n",
              "18   19.0         0.0         19\n",
              "19   20.0         2.0       Buzz\n",
              "20   21.0         1.0       Fizz\n",
              "21   22.0         0.0         22\n",
              "22   23.0         0.0         23\n",
              "23   24.0         1.0       Fizz\n",
              "24   25.0         2.0       Buzz\n",
              "25   26.0         0.0         26\n",
              "26   27.0         1.0       Fizz\n",
              "27   28.0         0.0         28\n",
              "28   29.0         0.0         29\n",
              "29   30.0         3.0  Fizz-Buzz\n",
              "..    ...         ...        ...\n",
              "70   71.0         0.0         71\n",
              "71   72.0         1.0       Fizz\n",
              "72   73.0         0.0         73\n",
              "73   74.0         0.0         74\n",
              "74   75.0         3.0  Fizz-Buzz\n",
              "75   76.0         0.0         76\n",
              "76   77.0         0.0         77\n",
              "77   78.0         1.0       Fizz\n",
              "78   79.0         0.0         79\n",
              "79   80.0         2.0       Buzz\n",
              "80   81.0         1.0       Fizz\n",
              "81   82.0         0.0         82\n",
              "82   83.0         0.0         83\n",
              "83   84.0         1.0       Fizz\n",
              "84   85.0         2.0       Buzz\n",
              "85   86.0         0.0         86\n",
              "86   87.0         1.0       Fizz\n",
              "87   88.0         0.0         88\n",
              "88   89.0         0.0         89\n",
              "89   90.0         3.0  Fizz-Buzz\n",
              "90   91.0         0.0         91\n",
              "91   92.0         0.0         92\n",
              "92   93.0         1.0       Fizz\n",
              "93   94.0         0.0         94\n",
              "94   95.0         2.0       Buzz\n",
              "95   96.0         1.0       Fizz\n",
              "96   97.0         0.0         97\n",
              "97   98.0         0.0         98\n",
              "98   99.0         1.0       Fizz\n",
              "99  100.0         2.0       Buzz\n",
              "\n",
              "[100 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "metadata": {
        "id": "gh8z3Iai0lxR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "                                                                                                                                             **   .................**......................**"
      ]
    }
  ]
}